import copy
import json
import datetime
from pathlib import Path
import re

# Pre-req
import docker
from docker import errors

# Our modules
import helpers
import core
import __version__


int_severity_order = {}
json_filters = {}


def check_image_in_public_registry(target_image):
    """
    Check if the target image exists in a public registry.
    :param target_image: 'Target image' in format registry/repo/image:tag; repo/image:tag; image:tag; repo/image
    :return: Tuple with image name and sha256 digest
    """
    # Matches: repo/image:tag; image:tag; repo/image
    # Won't match: registry/repo/image:tag
    # Ref: https://regex101.com/r/a98UqN/1
    pattern = r'^(?P<repository>[\w.\-_]+((?::\d+|)(?=/[a-z0-9._-]+/[a-z0-9._-]+))|)(?:/|)(?P<image>[a-z0-9.\-_]+' \
              r'(?:/[a-z0-9.\-_]+|))(:(?P<tag>[\w.\-_]{1,127})|)$'
    match = re.match(pattern, target_image)

    if match is not None:
        dc = docker.from_env()
        try:
            if match.group('tag'):
                res = dc.images.get_registry_data("{}:{}".format(match.group('image'), match.group('tag')))
            else:
                res = dc.images.get_registry_data("{}".format(match.group('image')))
            return res.image_name, res.id

        except docker.errors.APIError:
            # Maybe this image is not public.
            return False
    else:
        core.log_and_print("debug", "{} does not match a public image.".format(target_image))
        return False


def check_image_in_server_list(target_image):
    dc = docker.from_env()
    try:
        res = dc.images.list(target_image)
        return res[0].attrs["RepoTags"][0], str(res[0].attrs["RepoDigests"][0]).split('@')[1]

    except docker.errors.APIError as e:
        print("error", "{}".format(e.args))
        return False

    except IndexError:
        print("error", "Could not find image '{}'".format(target_image))
        return False


def filtered_match(json_vuln):
    """
    Matches vulnerability JSON with entries from filter JSON. All matches are excluded from final report.
    First match in the filter list wins. To be a match, a vulnerability must match all conditions in the field.
    :param json_vuln: JSON object with vulnerability attributes from the tools
    :return: True if one of the filter entries match; False if none matched
    """
    global json_filters, int_severity_order
    if json_filters == {}:
        # We don't want to load the filters for every vulnerability of every target image
        json_filters = load_from_json(core.config["reporting"]["filters_file"])
        # Same for the severity field order; Can use 'float' numbers
        int_severity_order = load_from_json("severity.conf.json")

    matched = False
    # Condition between filters: OR => One filter_entry must match
    for filter_entry in json_filters:
        # A filter_entry is a dict in the list
        # Condition between fields: AND => All fields in the filter_entry must match
        severity_match = False
        try:
            # Severity matches if it is lesser than or equal to filter; Can use 'float' numbers
            severity_match = \
                int_severity_order.get(json_vuln["severity"]) <= int_severity_order.get(filter_entry["severity"])

        except KeyError:
            # There is no "severity" key for this filter entry, but it is not mandatory
            pass

        for k in filter_entry:
            try:
                if (k == "severity" and severity_match) or (filter_entry[k] == json_vuln[k] and k != "severity"):
                    # Severity matches or field value matches the filter_entry
                    matched = True
                else:
                    # No match for this key (we change the status, even if there was a match before, for another key)
                    matched = False

            except KeyError:
                # Probably a wrong key in the filter; we will let the user know about it
                core.log_and_print("error", "Could not understand filter '{}'. Check file {}"
                                   .format(k, core.config["reporting"]["filters_file"]))

        # If matched, return True; if not, continue matching the other filter entries
        if matched:
            return True

    return False


def parse_json_scanner_to_sinker(target_image, parsed_json_object, scanner_name):
    """
    This function creates the JSON structure for each vulnerability found by the scanner inside Sinker JSON.
    :param target_image: registry, repo, target image repo and tag
    :param parsed_json_object: JSON object generated by parsing functions
    :param scanner_name: String, scanner name
    :return: True
    """
    # Load filters and match parsed vulnerabilities
    if filtered_match(parsed_json_object):
        #  Found a match, not going to include this vulnerability in the report
        return

    vuln_id = parsed_json_object["id"]
    pkg_name = parsed_json_object["artifact"]
    severity = parsed_json_object["severity"]

    # By scanner:
    if core.report_by_scanner:
        try:
            # This section is for one scanner only, so it doesn't need "found_by" field
            json_copy = copy.deepcopy(parsed_json_object)
            json_copy.pop("found_by")
            try:
                core.json_sinker["reporting"][target_image]["by_scanner"][scanner_name].append(json_copy)

            except KeyError:
                core.json_sinker["reporting"][target_image]["by_scanner"][scanner_name] = {}
                core.json_sinker["reporting"][target_image]["by_scanner"][scanner_name].append(json_copy)

        except KeyError:
            # Should have been created in create_json_structure()
            pass

    # By ID (this one will always be generated - will be used in other sections):
    try:
        # my = core.json_sinker["reporting"][target_image]["by_id"]
        assert core.json_sinker["reporting"][target_image]["by_id"]

    except AssertionError:
        core.json_sinker["reporting"][target_image]["by_id"] = []

    finally:
        # Before appending, we must locate if this 'ID and artifact' already exist
        found = -1
        for i, item in enumerate(core.json_sinker["reporting"][target_image]["by_id"]):
            # We already found that ID for the same artifact (for that target image)
            if vuln_id in item["id"] and pkg_name in item["artifact"]:
                found = i
                if scanner_name not in core.json_sinker["reporting"][target_image]["by_id"][i]["found_by"]:
                    # It was found by another scanner, let's add our name to it as well
                    core.json_sinker["reporting"][target_image]["by_id"][i]["found_by"].append(scanner_name)
                    break
        if found == -1:  # Not found
            core.json_sinker["reporting"][target_image]["by_id"].append(parsed_json_object)

    # By Severity:
    if core.report_by_severity:
        try:
            # my = core.json_sinker["reporting"][target_image]["by_severity"][severity]
            assert core.json_sinker["reporting"][target_image]["by_severity"][severity]

        except AssertionError:
            # Should have been created in create_json_structure()
            core.json_sinker["reporting"][target_image]["by_severity"][severity] = []

        finally:
            # Before appending, we must locate if this 'ID and artifact' already exist for this severity
            found = -1
            for i, item in enumerate(core.json_sinker["reporting"][target_image]["by_severity"][severity]):
                # We already found that ID for the same artifact with that severity
                if vuln_id in item["id"] and pkg_name in item["artifact"] and scanner_name not in \
                        core.json_sinker["reporting"][target_image]["by_severity"][severity][i]["found_by"]:
                    core.json_sinker["reporting"][target_image]["by_severity"][severity][i]["found_by"] \
                        .append(scanner_name)
                    found = i
                    break
            if found == -1:
                core.json_sinker["reporting"][target_image]["by_severity"][severity] \
                    .append(parsed_json_object)

    # By Artifact:
    if core.report_by_artifact:
        try:
            assert core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name]
            # my = core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name]

        except AssertionError:
            # We haven't found this Artifact yet
            core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name] = []

        finally:
            # Before appending, we must locate if this 'ID' already exist for this artifact
            found = -1
            for i, item in enumerate(core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name]):
                # We already found that ID for the same artifact with that severity
                if vuln_id in item["id"] and scanner_name not in \
                        core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name][i]["found_by"]:
                    core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name][i]["found_by"] \
                        .append(scanner_name)
                    found = i
                    break
            if found == -1:
                core.json_sinker["reporting"][target_image]["by_artifact"][pkg_name] \
                    .append(parsed_json_object)


def process_grype(json_object, json_filename, target_image, total_time):
    """
    Processes Grype JSON output and collects summary data.
    Tested with Grype v0.34.7.
    :param json_object: JSON data loaded from file
    :param json_filename: The name of the file that contained the data
    :param target_image: Target image name and tag
    :param total_time: The time it took to run Grype
    :return: True if JSON structure was readable (and parsed), False otherwise
    """
    # If we failed to load the file as a JSON object (in function load_from_json()), stop
    if not json_object:
        return

    int_count_vulnerabilities, int_critical, int_high, int_medium, int_low, int_unknown, int_negligible = \
        0, 0, 0, 0, 0, 0, 0
    try:
        # Grype version
        grype_version = json_object["descriptor"]["version"]

        # Collecting info per vuln found
        for vuln in json_object["matches"]:
            int_count_vulnerabilities += 1

            vuln_id = vuln["vulnerability"]["id"]
            pkg_name = vuln["artifact"]["name"]
            version = vuln["artifact"]["version"]
            fixed_state = vuln["vulnerability"]["fix"]["state"]
            fixed_in = vuln["vulnerability"]["fix"]["versions"]

            try:
                upstream = vuln["artifact"]["upstreams"][0]["name"]

            except IndexError:
                #  No "0", list is empty. There may be no upstream, this is not a dependency.
                upstream = ""

            except KeyError:
                # No key "name".
                upstream = ""

            # To avoid "UnboundLocalError: local variable 'cvss_v3score' referenced before assignment"
            # I'm defining them here, in case we cannot process JSON file as we expect
            cvss_v2score, cvss_v3score = 0, 0
            cvss_v2vector, cvss_v3vector = None, None

            # Based on the cases I've seen:
            # Vuln top node contains a MITRE link and might have a 'description' (if there's no 'relatedVulnerability')
            # When there is a 'relatedVulnerability' item, they put the 'description' there and
            # the 'dataSource' refers to NVD.

            # Links to vendor (library) and OS distributions are placed in 'relatedVulnerabilities.urls' list.
            # I think the vendor understands better the impact of the vulnerability in that artifact than NIST NVD,
            # because it uses a more broad definition and score for all affected artifacts.
            # Due to the infinite number of possible vendors and the many references in 'relatedVulnerabilities.urls',
            # it is not possible to determine (with certainty) which one is the vendor.

            # Current solution:
            # Grype usually puts this information in fields:
            # 'dataSource' points to MITRE
            # 'relatedVulnerabilities.dataSource' points to NIST NVD
            # First item of 'relatedVulnerabilities.urls' list (usually) has a link to the vendor website.
            try:
                description = vuln["relatedVulnerabilities"][0]["description"]
                datasource_url = vuln["relatedVulnerabilities"][0]["urls"][0]
                datasource_nvd_url = vuln["relatedVulnerabilities"][0]["dataSource"]
                datasource_mitre_url = ""
                for ref in vuln["vulnerability"]["urls"]:
                    if "mitre.org" in ref:
                        datasource_mitre_url = ref
                        break
                for item in vuln["relatedVulnerabilities"][0]["cvss"]:
                    if float(item["version"]) >= 3.1:
                        cvss_v3score = item["metrics"]["baseScore"]
                        cvss_v3vector = item["vector"]
                    elif item["version"] == "2.0":
                        cvss_v2score = item["metrics"]["baseScore"]
                        cvss_v2vector = item["vector"]
                    else:
                        cvss_v2score, cvss_v3score = 0, 0
                        cvss_v2vector, cvss_v3vector = None, None

            except IndexError:
                # There is no 'relatedVulnerability', then those fields exist in the top node
                try:
                    # Actually, they might not exist in the top node. They might not exist at all.
                    description = vuln["vulnerability"]["description"]

                except KeyError:
                    # If there is no other source for description...
                    description = ""

                datasource_url = vuln["vulnerability"]["dataSource"]
                datasource_nvd_url = vuln["vulnerability"]["dataSource"]
                datasource_mitre_url = ""
                for ref in vuln["vulnerability"]["urls"]:
                    if "mitre.org" in ref:
                        datasource_mitre_url = ref
                        break

                try:
                    # There might be no CVSS at all, just empty lists in 'vulnerability' and 'relatedVulnerability'
                    for item in vuln["vulnerability"]["cvss"]:
                        if float(item["version"]) >= 3.1:
                            cvss_v3score = item["metrics"]["baseScore"]
                            cvss_v3vector = item["vector"]
                        elif item["version"] == "2.0":
                            cvss_v2score = item["metrics"]["baseScore"]
                            cvss_v2vector = item["vector"]
                        else:
                            cvss_v2score, cvss_v3score = 0, 0
                            cvss_v2vector, cvss_v3vector = None, None

                except IndexError:
                    cvss_v3score = 0
                    cvss_v3vector = None

            # Collect 'severity' and increase the counters
            severity = str(vuln["vulnerability"]["severity"]).lower()
            if severity == "critical":
                int_critical += 1
            elif severity == "high":
                int_high += 1
            elif severity == "medium":
                int_medium += 1
            elif severity == "low":
                int_low += 1
            elif severity == "unknown":
                int_unknown += 1
            elif severity == "negligible":
                int_negligible += 1

            # This is the JSON item crafted to be appended in the reporting section
            # For completeness, it will contain all the fields, which means that
            # reporting by ID will also include ID in the vulnerability item,
            # as reporting by severity will include the severity in the item
            json_grype_all_fields = {
                "id": vuln_id,
                "description": description,
                "artifact": pkg_name,
                "upstream": upstream,
                "severity": severity,
                "installed_version": version,
                "fixed_version": fixed_in,
                "cvss_v2score": cvss_v2score,
                "cvss_v2vector": cvss_v2vector,
                "cvss_v3score": cvss_v3score,
                "cvss_v3vector": cvss_v3vector,
                "datasource": datasource_url,
                "datasource_nvd": datasource_nvd_url,
                "datasource_mitre": datasource_mitre_url,
                "found_by": ["grype"]
            }

            parse_json_scanner_to_sinker(target_image, json_grype_all_fields, "grype")

        # Pack Grype summary for this image
        json_grype_summary = {
            "json_file": json_filename,
            "version": grype_version,  # core.get_version_grype(core.config),
            "running_time": total_time,
            "findings": int_count_vulnerabilities,
            "findings_by_severity": {
                "critical": int_critical,
                "high": int_high,
                "medium": int_medium,
                "low": int_low,
                "unknown": int_unknown,
                "negligible": int_negligible
            }
        }

        # Save Grype summary anyway
        core.json_sinker["reporting"][target_image]["summary_by_scanner"]["grype"] = {}
        core.json_sinker["reporting"][target_image]["summary_by_scanner"]["grype"].update(json_grype_summary)

    except TypeError:
        try:
            # Grype version
            grype_version = json_object["descriptor"]["version"]

            if not json_object["Results"][0]["Vulnerabilities"]:
                json_grype_summary = {
                    "json_file": json_filename,
                    "version": grype_version,  # core.get_version_grype(core.config),
                    "running_time": total_time,
                    "findings": int_count_vulnerabilities
                }
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["grype"] = {}
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["grype"].update(json_grype_summary)

                core.log_and_print("warn", "Grype did not process this application.")

        except KeyError:
            core.log_and_print("error", "Could not process this file: {}".format(json_filename))


def process_trivy(json_object, json_filename, target_image, total_time):
    """
    Processes Trivy JSON output and collects summary data.
    Tested with Trivy v0.24.1, which is very buggy.
    :param json_object: JSON data loaded from file
    :param json_filename: The name of the file that contained the data
    :param target_image: Target image name and tag
    :param total_time: The time it took to run Trivy
    :return: True if JSON structure was readable (and parsed), False otherwise
    """
    # If we failed to load the file as a JSON object (in function load_from_json()), stop
    if not json_object:
        return

    int_count_vulnerabilities, int_critical, int_high, int_medium, int_low, int_unknown, int_negligible = \
        0, 0, 0, 0, 0, 0, 0
    try:
        try:
            int_count_vulnerabilities = len(json_object["Results"][0]["Vulnerabilities"])

        except KeyError:
            # If there are no reported vulnerabilities, this key won't exist
            # We pass, then int_count_vulnerabilities will continue to be == 0
            pass

        # If at least one vuln was reported
        if int_count_vulnerabilities > 0:
            # Collecting info for that vulnerability in the list
            for vuln in json_object["Results"][0]["Vulnerabilities"]:
                vuln_id = vuln["VulnerabilityID"]
                try:
                    description = vuln["Description"]

                except KeyError:
                    # Found vulnerabilities without any description
                    description = ""

                pkg_name = vuln["PkgName"]
                version = vuln["InstalledVersion"]
                try:
                    fixed_in = vuln["FixedVersion"]

                except KeyError:
                    # If there is no fix, Trivy won't even include the field
                    fixed_in = ""

                # Trivy does not include any info regarding upstream artifact
                upstream = ""

                cvss_v2score, cvss_v3score = 0, 0
                cvss_v2vector, cvss_v3vector = None, None

                # CVSSv3: First choice is 'redhat', if available. NVD is the fallback option.
                try:
                    cvss_v3score = vuln["CVSS"]["redhat"]["V3Score"]
                    cvss_v3vector = vuln["CVSS"]["redhat"]["V3Vector"]

                except KeyError:
                    try:
                        cvss_v3score = vuln["CVSS"]["nvd"]["V3Score"]
                        cvss_v3vector = vuln["CVSS"]["nvd"]["V3Vector"]

                    except KeyError:
                        # We might not have CVSSv3 at all. Nothing can be done.
                        pass

                # CVSSv2: First choice is 'redhat', if available. NVD is the fallback option.
                try:
                    cvss_v2score = vuln["CVSS"]["redhat"]["V2Score"]
                    cvss_v2vector = vuln["CVSS"]["redhat"]["V2Vector"]

                except KeyError:
                    try:
                        cvss_v2score = vuln["CVSS"]["nvd"]["V2Score"]
                        cvss_v2vector = vuln["CVSS"]["nvd"]["V2Vector"]

                    except KeyError:
                        # We might not have CVSSv2 at all. Nothing can be done.
                        pass

                # 'DataSource' section refers to vendor website, but it is not specific
                # 'PrimaryURL' refers to NVD references inside Aqua website
                # First item of 'References' list (usually) has a specific link to the vendor website.
                try:
                    datasource_url = vuln["References"][0]
                except IndexError:
                    datasource_url = vuln["DataSource"]["URL"]
                except KeyError:
                    # There is no "References" for this vulnerability
                    datasource_url = vuln["DataSource"]["URL"]

                datasource_nvd_url = vuln["PrimaryURL"]
                datasource_mitre_url = ""
                try:
                    for ref in vuln["References"]:
                        if "mitre.org" in ref:
                            datasource_mitre_url = ref
                            break
                except KeyError:
                    # There is no "References" for this vulnerability
                    # datasource_mitre_url will continue to be == ""
                    pass

                # Collect 'severity' and increase the counters
                severity = str(vuln["Severity"]).lower()    # Trivy JSON file writes severities in uppercase
                if severity == "critical":
                    int_critical += 1
                elif severity == "high":
                    int_high += 1
                elif severity == "medium":
                    int_medium += 1
                elif severity == "low":
                    int_low += 1
                elif severity == "unknown":
                    int_unknown += 1
                elif severity == "negligible":
                    int_negligible += 1

                # This is the JSON item crafted to be appended in the reporting section
                # For completeness, it will contain all the fields, which means that
                # reporting by ID will also include ID in the vulnerability item,
                # as reporting by severity will include the severity in the item
                json_trivy_all_fields = {
                    "id": vuln_id,
                    "description": description,
                    "artifact": pkg_name,
                    "upstream": upstream,
                    "severity": severity,
                    "installed_version": version,
                    "fixed_version": fixed_in,
                    "cvss_v2score": cvss_v2score,
                    "cvss_v2vector": cvss_v2vector,
                    "cvss_v3score": cvss_v3score,
                    "cvss_v3vector": cvss_v3vector,
                    "datasource": datasource_url,
                    "datasource_nvd": datasource_nvd_url,
                    "datasource_mitre": datasource_mitre_url,
                    "found_by": ["trivy"]
                }

                parse_json_scanner_to_sinker(target_image, json_trivy_all_fields, "trivy")

            # Pack Trivy summary for this image
            json_trivy_summary = {
                "json_file": json_filename,
                "version": core.version_trivy,
                "running_time": total_time,
                "findings": int_count_vulnerabilities,
                "findings_by_severity": {
                    "critical": int_critical,
                    "high": int_high,
                    "medium": int_medium,
                    "low": int_low,
                    "unknown": int_unknown,
                    "negligible": int_negligible
                }
            }

            # Save Trivy summary anyway
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"] = {}
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"].update(json_trivy_summary)

        else:
            # int_count_vulnerabilities = 0
            json_trivy_summary = {
                "json_file": json_filename,
                "version": core.version_trivy,
                "running_time": total_time,
                "findings": int_count_vulnerabilities
            }
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"] = {}
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"].update(json_trivy_summary)

    except TypeError:
        try:
            if not json_object["Results"][0]["Vulnerabilities"]:
                json_trivy_summary = {
                    "json_file": json_filename,
                    "version": core.version_trivy,
                    "running_time": total_time,
                    "findings": int_count_vulnerabilities
                }
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"] = {}
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["trivy"].update(json_trivy_summary)

                core.log_and_print("warn", "Trivy did not process this application.")

        except KeyError:
            core.log_and_print("error", "Could not process this file: {}".format(json_filename))


def process_snyk(json_object, json_filename, target_image, total_time):
    """
    Processes Snyk JSON output and collects summary data.
    Tested with Snyk v1.883.0 (standalone).
    :param json_object: JSON data loaded from file
    :param json_filename: The name of the file that contained the data
    :param target_image: Target image name and tag
    :param total_time: The time it took to run Snyk
    :return: True if JSON structure was readable (and parsed), False otherwise
    """
    # If we failed to load the file as a JSON object (in function load_from_json()), stop
    if not json_object:
        return

    int_count_vulnerabilities, int_critical, int_high, int_medium, int_low, int_unknown, int_negligible = \
        0, 0, 0, 0, 0, 0, 0
    try:
        try:
            int_count_vulnerabilities = len(json_object["vulnerabilities"])

        except KeyError:
            # If there are no reported vulnerabilities, this key won't exist
            # We pass, then int_count_vulnerabilities will continue to be == 0
            pass

        # If at least one vuln was reported
        if int_count_vulnerabilities > 0:
            # Collecting info for that vulnerability in the list
            for vuln in json_object["vulnerabilities"]:
                try:
                    vuln_id = vuln["identifiers"]["CVE"][0]
                except KeyError:
                    vuln_id = vuln["identifiers"]["CWE"][0]

                description = vuln["description"]

                # Snyk concatenates package and dependency in format "package/dependency" in node "packageName"
                # Other scanners, like Trivy and Grype, just use "dependency" for package name
                # Grype includes a reference for the upstream package in "artifact.upstream" subnode
                # Snyk does it in the "name" node
                name = vuln["name"]
                pkg_name = vuln["packageName"]
                upstream = ""
                if pkg_name in name:
                    try:
                        # If Snyk might be using "package/dependency", let's try to split them
                        pkg_name = str(name).split('/')[1]
                        upstream = str(name).split('/')[0]

                    except IndexError:
                        # It is not. Probably "name" does not contain a dependency.
                        upstream = name

                version = vuln["version"]
                try:
                    fixed_in = vuln["nearestFixedInVersion"]

                except KeyError:
                    fixed_in = ""

                cvss_v3score = vuln["cvssScore"]
                cvss_v3vector = vuln["CVSSv3"]

                datasource_nvd_url = ""
                datasource_mitre_url = ""
                datasource_url = ""
                for ref in vuln["references"]:
                    if "MISC" in ref["title"] or "N/A" in ref["title"] or "CONFIRM" in ref["title"]:
                        datasource_url = ref["url"]
                        break

                # Collect 'severity' and increase the counters
                severity = str(vuln["severity"]).lower()    # Snyk JSON file writes severities in uppercase
                if severity == "critical":
                    int_critical += 1
                elif severity == "high":
                    int_high += 1
                elif severity == "medium":
                    int_medium += 1
                elif severity == "low":
                    int_low += 1
                elif severity == "unknown":
                    int_unknown += 1
                elif severity == "negligible":
                    int_negligible += 1

                # This is the JSON item crafted to be appended in the reporting section
                # For completeness, it will contain all the fields, which means that
                # reporting by ID will also include ID in the vulnerability item,
                # as reporting by severity will include the severity in the item
                json_snyk_all_fields = {
                    "id": vuln_id,
                    "description": description,
                    "artifact": pkg_name,
                    "upstream": upstream,
                    "severity": severity,
                    "installed_version": version,
                    "fixed_version": fixed_in,
                    "cvss_v2score": None,   # Not included in Snyk report
                    "cvss_v2vector": None,  # Not included in Snyk report
                    "cvss_v3score": cvss_v3score,
                    "cvss_v3vector": cvss_v3vector,
                    "datasource": datasource_url,
                    "datasource_nvd": datasource_nvd_url,
                    "datasource_mitre": datasource_mitre_url,
                    "found_by": ["snyk"]
                }

                parse_json_scanner_to_sinker(target_image, json_snyk_all_fields, "snyk")

            # Pack Snyk summary for this image
            json_snyk_summary = {
                "json_file": json_filename,
                "version": core.version_snyk,
                "running_time": total_time,
                "findings": int_count_vulnerabilities,
                "findings_by_severity": {
                    "critical": int_critical,
                    "high": int_high,
                    "medium": int_medium,
                    "low": int_low,
                    "unknown": int_unknown,
                    "negligible": int_negligible
                }
            }

            # Save Snyk summary anyway
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"] = {}
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"].update(json_snyk_summary)

        else:
            json_snyk_summary = {
                "json_file": json_filename,
                "version": core.version_snyk,
                "running_time": total_time,
                "findings": int_count_vulnerabilities
            }
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"] = {}
            core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"].update(json_snyk_summary)

    except TypeError:
        try:
            if not json_object["vulnerabilities"]:
                json_snyk_summary = {
                    "json_file": json_filename,
                    "version": core.version_snyk,
                    "running_time": total_time,
                    "findings": int_count_vulnerabilities
                }
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"] = {}
                core.json_sinker["reporting"][target_image]["summary_by_scanner"]["snyk"].update(json_snyk_summary)

                core.log_and_print("warn", "Snyk did not process this application.")

        except KeyError:
            core.log_and_print("error", "Could not process this file: {}".format(json_filename))


def process_snyk_image_rec(snyk_json, target_image):
    """
    After processing each vulnerability found, looks for a recommendation for the image
    :param snyk_json: Full Snyk report from the tool
    :param target_image: registry/repo/image:tag for the target image being analysed
    :return: True
    """
    upgrade_message, upgrade_target = "", None

    # If Snyk didn't have network access, it will return a JSON with { "ok": false, "error": "..." }
    try:
        if not snyk_json["ok"] and snyk_json["error"]:
            if "authentication token has been revoked" in snyk_json["error"]:
                core.log_and_print("error", "Could not run Snyk on {}. Authentication token has been revoked."
                                   .format(target_image))
            else:
                core.log_and_print("error", "Could not run Snyk on {}. Network error?".format(target_image))
            return

    except KeyError:
        # There is no "error" field, meaning execution was ok. We can continue.
        pass

    # No vulnerability data
    if snyk_json["docker"]["baseImageRemediation"]["code"] == "SCRATCH_BASE_IMAGE":
        upgrade_message = snyk_json["docker"]["baseImageRemediation"]["advice"][0]["message"]

    # No remediation
    if snyk_json["docker"]["baseImageRemediation"]["code"] == "NO_REMEDIATION_AVAILABLE" and \
            "using the most secure version" in snyk_json["docker"]["baseImageRemediation"]["advice"][0]["message"]:
        base_image = snyk_json["docker"]["baseImage"]
        upgrade_message = snyk_json["docker"]["baseImageRemediation"]["advice"][0]["message"]

    # Remediation available
    elif snyk_json["docker"]["baseImageRemediation"]["code"] == "REMEDIATION_AVAILABLE":
        base_image = snyk_json["docker"]["baseImage"]
        for item in snyk_json["docker"]["baseImageRemediation"]["advice"]:
            try:
                if item["bold"]:
                    if "Recommendations" not in item["message"]:
                        upgrade_message = item["message"]

            except KeyError:
                if base_image not in item["message"]:
                    upgrade_target = item["message"]

    try:
        assert core.json_sinker["reporting"][target_image]["recommendation"]

    except (KeyError, AssertionError):
        core.json_sinker["reporting"][target_image]["recommendation"] = {}
        core.json_sinker["reporting"][target_image]["recommendation"]["message"] = upgrade_message
        core.json_sinker["reporting"][target_image]["recommendation"]["target"] = upgrade_target


def load_from_json(json_filename):
    """
    Loads a text file into a JSON object.
    Called by process_snyk(), process_grype() and process_trivy().
    :param json_filename: JSON file to be processed
    :return: JSON object to passed to other functions; or False if there was an exception
    """
    try:
        with open(json_filename, 'r', encoding='utf-8') as f:
            try:
                res = json.loads(f.read())
                return res

            except json.decoder.JSONDecodeError:
                core.log_and_print("error", "Could not open {} as JSON data file.".format(json_filename))
                return False

    except FileNotFoundError:
        core.log_and_print("error", "Could not find file {}.".format(json_filename))
        return False


def write_json_to_file(data, sinker_output):
    """
    Writes JSON dict to file.
    Called by main().
    :param data: JSON object
    :param sinker_output: Destination where data will be written
    :return: True for success, otherwise exits with error code in case of exception
    """
    # Clean up destination path
    bad_chars = "\'\""
    full_dest = core.sinker_output_folder + '/' + sinker_output
    for c in bad_chars:
        full_dest = full_dest.replace(c, "")
    full_dest = full_dest.replace("//", "/")

    date_format = "%Y-%m-%d %H:%M:%S"
    data["sinker"]["end_time"] = str(datetime.datetime.now().strftime(date_format))
    dt_duration = datetime.datetime.strptime(data["sinker"]["end_time"], date_format) - \
        datetime.datetime.strptime(data["sinker"]["start_time"], date_format)
    data["sinker"]["duration_seconds"] = int(dt_duration.total_seconds())
    data["sinker"]["extended_duration"] = display_time(int(dt_duration.total_seconds()))

    try:
        Path(core.sinker_output_folder).mkdir(parents=True, exist_ok=True)

        with open(full_dest, 'w') as f:
            json.dump(data, f, indent=4)
            core.log_and_print("info", "Sinker JSON saved to {}".format(full_dest))

    except PermissionError:
        helpers.dest_not_writable(full_dest)


def display_time(seconds, granularity=5):
    # Modified from:
    # https://stackoverflow.com/questions/4048651/python-function-to-convert-seconds-into-minutes-hours-and-days
    result = []
    intervals = (
        ('weeks', 604800),  # 60 * 60 * 24 * 7
        ('days', 86400),    # 60 * 60 * 24
        ('hours', 3600),    # 60 * 60
        ('minutes', 60),
        ('seconds', 1),
    )

    for name, count in intervals:
        value = seconds // count
        if value:
            if value >= 1:
                seconds -= value * count
                if value == 1:
                    name = name.rstrip('s')
                result.append("{} {}".format(value, name))
    return ', '.join(result[:granularity])


def create_json_structure():
    """
    Create a basic JSON object.
    Called by main().
    :return: True if main JSON object is successfully updated; otherwise, exits with error code
    """
    if core.bool_command_line_args:
        str_command_line_args = str(core.args)
    else:
        str_command_line_args = None

    try:
        dc = docker.from_env()
        if core.bool_docker_version:
            str_docker_version = dc.version()
        else:
            str_docker_version = None

    except docker.errors.APIError:
        core.log_and_print("error", "Error: Could not read version from Docker client.")
        str_docker_version = None

    # JSON result file, schema version 1
    data = {
        "sinker": {
            "script_version": str(__version__.__version__),
            "json_schema_version": "1",
            "url": "https://github.com/mmartins000/sinker",
            "command_line_args": str_command_line_args,
            "docker_version": str_docker_version,
            "start_time": str(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
            "end_time": ""
        },
        "reporting": {}
    }

    try:
        targets_object = core.get_targets(core.targets_file)

        for target in targets_object["images"]:
            json_target = {
                target: {
                    "summary": {
                        "findings": 0,
                        "findings_by_scanner": {},
                        "findings_by_severity": {
                            "critical": 0,
                            "high": 0,
                            "medium": 0,
                            "low": 0,
                            "negligible": 0,
                            "unknown": 0

                        },
                        "unanimous_findings": 0,
                        "disputed_findings": 0
                    },
                }
            }

            # Reporting by Id is always on - will be used in other sections
            json_target[target]["by_id"] = []

            if core.report_by_severity:
                json_target[target]["by_severity"] = {
                    "critical": [],
                    "high": [],
                    "medium": [],
                    "low": [],
                    "negligible": [],
                    "unknown": []
                }

            if core.report_by_artifact:
                json_target[target]["by_artifact"] = {}

            if core.report_by_scanner:
                json_target[target]["by_scanner"] = {}

            if core.report_unanimous_findings:
                json_target[target]["unanimous_findings"] = []

            if core.report_disputed_findings:
                json_target[target]["disputed_findings"] = []

            json_target[target]["summary_by_scanner"] = {}

            data["reporting"].update(json_target)

        core.json_sinker.update(data)
        return data

    except (TypeError, KeyError, IndexError):
        helpers.missing_targets(core.targets_file)


def summarise_stats(json_object):
    """
    Summarises execution stats (total findings, findings by severity, findings by scanner and by all scanners)
    :param json_object: Sinker JSON object
    :return: True
    """
    # How many scanners did we run?
    scanners_count = 0
    for target in json_object["reporting"]:
        # We take the first target we find in this section and count how many scanners we had
        scanners_count = len(json_object["reporting"][target]["summary_by_scanner"])
        break
    # How many targets did we have?
    targets_count = len(json_object["reporting"])

    json_object["sinker"]["scanners_count"] = scanners_count
    json_object["sinker"]["targets_count"] = targets_count

    # To avoid running several times, we run only once here
    core.get_scanner_versions(config_object=core.config)

    # Now, calculating stats per target:
    for target in json_object["reporting"]:
        # Now we store the scanner version in JSON report:
        if core.config["scanners"]["trivy"]:
            json_object["reporting"][target]["summary_by_scanner"]["trivy"]["version"] = core.version_trivy
        if core.config["scanners"]["snyk"]:
            json_object["reporting"][target]["summary_by_scanner"]["snyk"]["version"] = core.version_snyk

        int_critical, int_high, int_medium, int_low, int_negligible, int_unknown = 0, 0, 0, 0, 0, 0

        # How many vulnerabilities did each scanner find, for that target image?
        scanner_foundby_count = {}
        for scanner in core.config["scanners"]:
            scanner_foundby_count[scanner] = 0

        for item in json_object["reporting"][target]:
            if "by_id" in item:
                # Total findings
                json_object["reporting"][target]["summary"]["findings"] = \
                    len(json_object["reporting"][target][item])

                # Findings by all scanners
                for i, vuln in enumerate(json_object["reporting"][target][item]):
                    # Findings by severity
                    if json_object["reporting"][target][item][i]["severity"] == "critical":
                        int_critical += 1
                    elif json_object["reporting"][target][item][i]["severity"] == "high":
                        int_high += 1
                    elif json_object["reporting"][target][item][i]["severity"] == "medium":
                        int_medium += 1
                    elif json_object["reporting"][target][item][i]["severity"] == "low":
                        int_low += 1
                    elif json_object["reporting"][target][item][i]["severity"] == "negligible":
                        int_negligible += 1
                    elif json_object["reporting"][target][item][i]["severity"] == "unknown":
                        int_unknown += 1

                    # If 'found_by' contains all enabled scanners, unanimous finding
                    if len(json_object["reporting"][target][item][i]["found_by"]) == scanners_count:
                        json_object["reporting"][target]["summary"]["unanimous_findings"] += 1

                        if core.report_unanimous_findings:
                            json_object["reporting"][target]["unanimous_findings"].append(vuln)

                        # Findings by scanner
                        for scanner in scanner_foundby_count:
                            if scanner in json_object["reporting"][target][item][i]["found_by"]:
                                scanner_foundby_count[scanner] += 1

                    else:
                        # If 'found_by' contains less than all enabled scanners, disputed finding
                        json_object["reporting"][target]["summary"]["disputed_findings"] += 1

                        # Findings by scanner
                        for scanner in scanner_foundby_count:
                            if scanner in json_object["reporting"][target][item][i]["found_by"]:
                                scanner_foundby_count[scanner] += 1

                        if core.report_disputed_findings:
                            json_object["reporting"][target]["disputed_findings"].append(vuln)

                json_object["reporting"][target]["summary"]["findings_by_scanner"].update(scanner_foundby_count)

                json_object["reporting"][target]["summary"]["findings_by_severity"]["critical"] = int_critical
                json_object["reporting"][target]["summary"]["findings_by_severity"]["high"] = int_high
                json_object["reporting"][target]["summary"]["findings_by_severity"]["medium"] = int_medium
                json_object["reporting"][target]["summary"]["findings_by_severity"]["low"] = int_low
                json_object["reporting"][target]["summary"]["findings_by_severity"]["negligible"] = int_negligible
                json_object["reporting"][target]["summary"]["findings_by_severity"]["unknown"] = int_unknown

            elif "by_severity" in item:
                # Findings by severity
                json_object["reporting"][target]["summary"]["findings_by_severity"]["critical"] = \
                    len(json_object["reporting"][target][item]["critical"])
                json_object["reporting"][target]["summary"]["findings_by_severity"]["high"] = \
                    len(json_object["reporting"][target][item]["high"])
                json_object["reporting"][target]["summary"]["findings_by_severity"]["medium"] = \
                    len(json_object["reporting"][target][item]["medium"])
                json_object["reporting"][target]["summary"]["findings_by_severity"]["low"] = \
                    len(json_object["reporting"][target][item]["low"])
                json_object["reporting"][target]["summary"]["findings_by_severity"]["negligible"] = \
                    len(json_object["reporting"][target][item]["negligible"])
                json_object["reporting"][target]["summary"]["findings_by_severity"]["unknown"] = \
                    len(json_object["reporting"][target][item]["unknown"])

                # Total findings
                for severity in json_object["reporting"][target]["by_severity"]:
                    json_object["reporting"][target]["summary"]["findings"] += \
                        len(json_object["reporting"][target][item][severity])

                    # Findings by all scanners
                    for i, vuln in enumerate(json_object["reporting"][target][item][severity]):
                        if len(json_object["reporting"][target][item][severity][i]["found_by"]) == scanners_count:
                            json_object["reporting"][target]["summary"]["unanimous_findings"] += 1

                            if core.report_unanimous_findings:
                                json_object["reporting"][target]["unanimous_findings"].append(vuln)
                        else:
                            json_object["reporting"][target]["summary"]["disputed_findings"] += 1

                            if core.report_disputed_findings:
                                json_object["reporting"][target]["disputed_findings"].append(vuln)

                        # Findings by scanner
                        for scanner in scanner_foundby_count:
                            if scanner in json_object["reporting"][target][item][i]["found_by"]:
                                scanner_foundby_count[scanner] += 1

                json_object["reporting"][target]["summary"]["findings_by_scanner"].update(scanner_foundby_count)
